# FK Webstack - Project Organization Map

## Original Project (Docker Desktop)
**Status:** âœ… Complete on Docker Desktop

### Core Components
- **Frontend:** lighttpd (static HTML/JS) â†’ serves index.html with fetch() call to API
- **API:** Python FastAPI + Uvicorn + PyMongo â†’ REST endpoints, MongoDB queries
- **Database:** MongoDB 6+ â†’ stores profile data (name: "Frank Koch")
- **Deployment:** Docker Compose (`docker-compose.yaml`)

---

## Current Phase: Kubernetes Migration with Vagrant
**Status:** ğŸ”„ In Progress - Still on Step 1 of 4 features

### What We're Building
1. **3-Node Kubernetes Cluster** (kubeadm, Vagrant VMs, Flannel CNI)
   - 1 Control Plane (6GB RAM, 4 CPUs)
   - 2 Worker Nodes (3GB each, 2 CPUs each)
   - Run same 3 components (Frontend, API, MongoDB) as containerized pods

2. **Required Features to Add** (4 steps):
   - âœ… Step 1: Swap demo ingress to real API (STUCK - API pods CrashLoopBackOff)
   - â³ Step 2: Add healthchecks (blocked by Step 1)
   - â³ Step 3: Prometheus monitoring
   - â³ Step 4: ArgoCD + GitOps workflow

---

## File Organization - What to Keep âœ…

### **KEEP - Core Project Files**
```
WebserverLinux/
â”œâ”€â”€ Vagrantfile                 # 3-node cluster definition (WORKING)
â”‚
â”œâ”€â”€ api/                        # Source code (WORKING)
â”‚   â”œâ”€â”€ Dockerfile              # Container image for API
â”‚   â”œâ”€â”€ requirements.txt         # Python deps (fastapi, uvicorn, pymongo)
â”‚   â””â”€â”€ app/main.py             # FastAPI app (correct logic)
â”‚
â”œâ”€â”€ frontend/                   # Source code (WORKING)
â”‚   â”œâ”€â”€ Dockerfile              # Container image for frontend
â”‚   â”œâ”€â”€ lighttpd.conf           # Web server config
â”‚   â””â”€â”€ index.html              # Static HTML + fetch() to API
â”‚
â”œâ”€â”€ db/                         # Source code (WORKING)
â”‚   â””â”€â”€ init/
â”‚       â””â”€â”€ init.js             # MongoDB init script (sets name: "Frank Koch")
â”‚
â”œâ”€â”€ docker-compose.yaml         # Original setup (for reference)
â”‚
â”œâ”€â”€ k8s/                        # Kubernetes manifests (PARTIALLY WORKING)
â”‚   â”œâ”€â”€ 00-namespace.yaml       # âœ… namespace
â”‚   â”œâ”€â”€ 10-mongodb-deployment.yaml âœ…
â”‚   â”œâ”€â”€ 11-mongodb-service.yaml âœ…
â”‚   â”œâ”€â”€ 12-mongodb-init-configmap.yaml âœ…
â”‚   â”œâ”€â”€ 13-mongodb-init-job.yaml âœ…
â”‚   â”œâ”€â”€ 20-api-deployment.yaml  # âŒ BROKEN (pods CrashLoopBackOff)
â”‚   â”œâ”€â”€ 21-api-service.yaml     # âœ…
â”‚   â”œâ”€â”€ 22-api-hpa.yaml         # âœ… (ready for Step 2)
â”‚   â”œâ”€â”€ 30-frontend-deployment.yaml âœ…
â”‚   â”œâ”€â”€ 31-frontend-service.yaml âœ…
â”‚   â”œâ”€â”€ 40-ingress.yaml         # âœ… (but API backend down)
â”‚   â”œâ”€â”€ 50-cert-issuer.yaml     # ğŸ“‹ (for feature: HTTPS)
â”‚   â”œâ”€â”€ 51-selfsigned-issuer.yaml # ğŸ“‹ (for feature: HTTPS)
â”‚   â”œâ”€â”€ 60-argocd-application.yaml # ğŸ“‹ (for feature: GitOps)
â”‚
â”œâ”€â”€ vagrant/                    # Provisioning scripts (WORKING)
â”‚   â”œâ”€â”€ 01-base-setup.sh        # âœ… Ubuntu packages, Docker, containerd
â”‚   â”œâ”€â”€ 02-kubeadm-install.sh   # âœ… Kubernetes tooling
â”‚   â”œâ”€â”€ 03-control-plane-init.sh # âœ… kubeadm init, Flannel CNI
â”‚   â”œâ”€â”€ 04-worker-join.sh       # âœ… kubeadm join
â”‚   â”œâ”€â”€ 05-deploy-argocd.sh     # ğŸ“‹ (for feature: GitOps)
â”‚   â”œâ”€â”€ 06-build-images.sh      # âš ï¸ (needs review - Docker not stable on control)
â”‚   â””â”€â”€ README.md               # âœ…
â”‚
â”œâ”€â”€ docs/                       # Documentation (REFERENCE)
â”‚   â”œâ”€â”€ project-overview.md     # Complete project spec
â”‚   â”œâ”€â”€ DEPLOYMENT-RUNBOOK.md   # Step-by-step manual deployment
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md      # Issues and solutions
â”‚   â””â”€â”€ *.md (other docs)       # Various guides
â”‚
â””â”€â”€ AItext.txt                  # Project assignment (REFERENCE)
```

---

## FILE ORGANIZATION - DELETE/ARCHIVE âŒ

### **DELETE - Failed Attempts & Clutter**

#### Container Build Failures (many attempts):
```
build-and-distribute-images.sh          # âŒ Failed
build-images-and-push.sh                # âŒ Failed
deploy-with-nerdctl.sh                  # âŒ Failed
quick-deploy-on-control.sh              # âŒ Failed
setup-mongodb-api.sh                    # âŒ Failed
setup-real-api.sh                       # âŒ Failed
```
**Reason:** Control plane has no Docker daemon; containerd buildkit unstable; tried many workarounds

#### Multi-Script Deployment Confusion (overlapping attempts):
```
FINAL-DEPLOY.sh                         # âŒ Old attempt
MINIMAL-DEPLOY.sh                       # âŒ Old attempt
install-all.bat                         # âŒ Windows batch (abandoned)
install-all.ps1                         # âŒ PowerShell (abandoned)
install-stack.sh                        # âŒ Old shell attempt
deploy-all.ps1                          # âŒ PowerShell (abandoned)
deploy-incremental.sh                   # âŒ Incomplete
deploy-production.sh                    # âŒ Never used
deploy-direct-python.sh                 # âŒ Failed
```
**Reason:** Too many overlapping approaches; kept restarting from scratch

#### Puppet Installation (unnecessary complexity):
```
puppet/                                 # âŒ Entire folder (unused)
install-puppet-server.sh                # âŒ Not needed
install-puppet-agent.sh                 # âŒ Not needed
deploy-puppet-cluster.sh                # âŒ Not needed
sign-puppet-certs.sh                    # âŒ Not needed
PUPPET-GUIDE.md                         # âŒ Not needed
PUPPET-INSTALLATION-SUMMARY.md          # âŒ Not needed
```
**Reason:** Kubernetes is sufficient config management; Puppet overkill

#### Misc Failed/Abandoned Scripts:
```
check-cluster.sh                        # âŒ Incomplete
cleanup-obsolete-scripts.ps1            # âŒ Hasn't been run
emergency-recovery.sh                   # âŒ Never worked
fix-certmanager.sh                      # âŒ Incomplete
fix-worker1.sh                          # âŒ One-off workaround
full-worker-fix.sh                      # âŒ One-off workaround
flannel-simple.yaml                     # âŒ Old (Flannel now working)
apply-hostnet-stack.sh                  # âŒ Old (hostnet abandoned)
app-hostnet.yaml                        # âŒ Old (hostnet abandoned)
app-stack-control-plane.yaml            # âŒ Old attempt
simple-app-deploy.sh                    # âŒ Old demo
restart-k8s.sh                          # âŒ Incomplete
verify-all.sh                           # âš ï¸ Might be useful but old
quick-verify.sh                         # âš ï¸ Might be useful but old
```

#### Local Testing Scripts (not for K8s):
```
simple-web-server.py                    # âŒ Local testing
api-dual-server.py                      # âŒ Local testing
api-https-server.py                     # âŒ Local testing
api-mongodb-server.py                   # âŒ Local testing
init-mongodb.py                         # âŒ Local testing
quick-start-api.sh                      # âŒ Local testing
```
**Reason:** These are for Docker Desktop testing, not Kubernetes

#### Deployment Logs:
```
vagrant_deployment.log                  # âŒ Old logs
vagrant_deployment2.log                 # âŒ Old logs
vagrant_deployment3.log                 # âŒ Old logs
vagrant_deployment4.log                 # âŒ Old logs
```
**Reason:** Stale; superseded by new provisioning

#### Old/Incomplete Documentation:
```
README-TOMORROW.md                      # âŒ Incomplete stub
PUPPET-GUIDE.md                         # âŒ Not needed
PUPPET-INSTALLATION-SUMMARY.md          # âŒ Not needed
```

#### Kubernetes Attempts (failed approaches):
```
00-namespace.yaml (kept early, now stale) # âš ï¸ Check if still needed
kubeadm-config/join-command.sh          # âš ï¸ Auto-generated, can be regenerated
```

#### Git/Config Files (housekeeping):
```
.git/                                   # ğŸ”§ Git history (keep, but not project-critical)
.github/                                # âš ï¸ Check if used
.env.local.example                      # âš ï¸ Check if needed
kubeconfig.yaml                         # ğŸ”§ Remove - should be in ~/.kube/config
server.crt, server.key                  # ğŸ”§ Old certs (new ones generated by cert-manager)
```

---

## Current Reality Check

### What's Actually Working âœ…
- Vagrant cluster booting (3 VMs ready, networking OK)
- Flannel CNI networking (pods can reach each other)
- CoreDNS (DNS resolution working)
- NGINX Ingress Controller (accepting traffic on port 32685)
- MongoDB pod (1/1 Ready, test data inserted)
- Frontend pod (1/1 Ready, serving HTML on http://localhost/)
- Demo scaling app proved cross-node distribution works

### What's Broken âŒ
- **API Deployment:** Pods stuck in CrashLoopBackOff
  - **Root Cause:** ConfigMap YAML formatting corrupts requirements.txt file
  - **Impact:** pip install fails, container never reaches Ready state
  - **Blocker:** Step 1 (swap demo to real API)

---

## Recommended Cleanup Strategy

### Phase 1: Immediate Cleanup
1. **Archive failed scripts** â†’ Move to `_archive/` folder
2. **Delete Puppet** â†’ `rm -rf puppet install-puppet-*.sh PUPPET-*.md`
3. **Delete local testing scripts** â†’ Remove `*-server.py`, `init-mongodb.py`
4. **Delete old logs** â†’ Remove `vagrant_deployment*.log`

### Phase 2: Get Current Phase Working
1. **Fix Step 1:** Repair API deployment (ConfigMap issue)
   - Either: Fix ConfigMap text formatting
   - Or: Rebuild approach (pre-build images, push to registry)
2. **Test full stack:** Frontend â†” API â†” MongoDB

### Phase 3: Add Features
1. **Step 2:** Healthchecks (already in manifests, just enable)
2. **Step 3:** Prometheus (install + create scrape targets)
3. **Step 4:** ArgoCD (install + create Application)

---

## Quick Summary Table

| Component | Status | Location | Next Action |
|-----------|--------|----------|------------|
| **VMs (Vagrant)** | âœ… Working | Vagrantfile | None - ready |
| **Kubernetes (kubeadm)** | âœ… Working | vagrant/0*.sh | None - ready |
| **Flannel CNI** | âœ… Working | vagrant/03-*.sh | None - ready |
| **MongoDB** | âœ… Working | k8s/10-13-*.yaml | None - ready for data |
| **Frontend (nginx)** | âœ… Working | k8s/30-31-*.yaml + frontend/ | None - ready |
| **Ingress (NGINX)** | âœ… Working | k8s/40-ingress.yaml | Waiting for API |
| **API (FastAPI)** | âŒ Broken | k8s/20-21-*.yaml + api/ | ğŸ”´ FIX: ConfigMap corruption |
| **Healthchecks** | ğŸ“‹ Partial | k8s/20-*.yaml (has probes) | Enable once API works |
| **HPA** | ğŸ“‹ Ready | k8s/22-api-hpa.yaml | Deploy after Step 1 |
| **HTTPS/certs** | ğŸ“‹ Ready | k8s/50-51-*.yaml | Deploy for Step 3 |
| **Prometheus** | ğŸ“‹ Planned | vagrant/05-*.sh | Deploy for Step 3 |
| **ArgoCD** | ğŸ“‹ Planned | vagrant/05-*.sh + k8s/60-*.yaml | Deploy for Step 4 |

---

## Files You Can Delete Right Now

Copy-paste to delete from Windows PowerShell in workspace:

```powershell
# Remove failed build scripts
rm -Force build-and-distribute-images.sh, build-images-and-push.sh, deploy-with-nerdctl.sh
rm -Force quick-deploy-on-control.sh, setup-*.sh, deploy-direct-python.sh, deploy-production.sh

# Remove deployment confusion (use DEPLOYMENT-RUNBOOK.md instead)
rm -Force FINAL-DEPLOY.sh, MINIMAL-DEPLOY.sh, install-all.*, deploy-all.ps1, deploy-incremental.sh

# Remove Puppet
rm -Recurse -Force puppet/
rm -Force install-puppet-*.sh, deploy-puppet-cluster.sh, sign-puppet-certs.sh, PUPPET-*.md

# Remove local testing scripts (not for K8s)
rm -Force *-server.py, init-mongodb.py, quick-start-api.sh

# Remove old logs
rm -Force vagrant_deployment*.log

# Clean up old Kubernetes attempts
rm -Force flannel-simple.yaml, apply-hostnet-stack.sh, app-hostnet.yaml, app-stack-control-plane.yaml
rm -Force simple-app-deploy.sh, restart-k8s.sh, check-cluster.sh, emergency-recovery.sh, fix-*.sh

# Optional: old cert files (new ones from cert-manager)
rm -Force server.crt, server.key

# Optional: Misc stubs
rm -Force README-TOMORROW.md, .env.local.example
```

After cleanup, workspace will have only **what you actually need** for the Kubernetes migration.
